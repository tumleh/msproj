\documentclass{IEEEtran}%[conference]{IEEEtran}%\documentclass[10pt]{article}%{article}{report}{letter}{book}{proc}{slides}%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{wasysym}
\usepackage{skull}

\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue}

%\textwidth=5.5in
%\oddsidemargin=0.5in
%\evensidemargin=0.0in
%\textheight=8.0in
%\topmargin=0.0in



\begin{document}

\title{Delay Comparison of Different Switch Architectures}



% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
%\author{Stephan Adams and Libin Jiang}
%\IEEEauthorblockA{Department of Electrical Engineering and Computer Science\\
%University of California, Berkeley\\
%\{shadams, jnchang\}@eecs.berkeley.edu}
\author{\IEEEauthorblockN{S. H. Adams, L. Huang, A. Parekh, and J. Walrand \\}}
%\IEEEauthorblockA{Department of Electrical Engineering and Computer Science\\
%University of California, Berkeley\\
%\{shadams,jnchang\}@eecs.berkeley.edu}}


% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}

% make the title area
\maketitle


\begin{abstract}
	We explore the delay performance of alternatives to the state of the art switch designs.  In particular we look at variations on QCSMA \cite{Libin} invented by Libin Jiang and a switch architecture based on Benes networks \cite{Walrand_Varaiya}.  The state of the art switch designs that we compare with is the iterative SLIP algorithm invented by Nick McKeown \cite{McKeown}.  Our simulation results suggest that SLIP performs as well or better than the alternatives, in the settings considered.  Some of the assumptions made on the architecture constraints may skew the results in the favor of SLIP and our future work will focus on a more realistic simulation design to investigate this further.
\end{abstract}


\section{Introduction}
Datacenters are at the heart of the new internet.  The power of datacenters comes from the ability to distribute applications across many different machines to leverage distributed computation and improve response time.  In order to provide good latency for these distributed applications the communication between machines should be optimized.   As the demand for online resources increases, it is important to use datacenters at their maximum capacity.  The datacenter network is of crucial importance to support the low latency required by these applications.

While there have been many proposals for different topologies, one of the key aspects shared by all topologies has not been thoroughly examined in the datacenter literature.  The switches and routers that connect the different branches of the chosen topology should run as efficiently as possible to ensure that we can leverage the network to its fullest potential.  Ideally we would like to ensure that these connection points do not become a bottleneck to network performance.

  Currently the state of the art switch is a crossbar scheduled with Nick Mckeown's iterative SLIP algorithm \cite{McKeown}.  SLIP performs very well in simulations but does not provide the strong guarantees on throughput that other switch designs such as QCSMA (invented and proven throughput optimal by Libin Jiang \cite{Libin}) or some Benes network inspired switches \cite{Walrand_Varaiya} can provide -- {Both of which provide queuing theoretic throughput optimality.}   Our goal is to carefully examine these new designs to determine if it is possible to improve on the currently used switches.

Although proofs on optimality are limited to the throughput performance of these switches, the metric of interest in datacenter environments is delay, which is very difficult to analyze.  In order to compare the different switch designs, we have built a simulator to provide a framework for measuring the switch performance side by side, and explore what practical benefits we can expect from implementing these alternative switch designs.

%  in low latency applications such as user queries, the quick response Network is a really important piece of a datacenter.  
%How to use it best?  
%Topologies need to allow for good accessibility, but switches need to be able to handle the demands of the system well and efficiently.
%Metrics considered are throughput (to support a lot of traffic) and delay (since latency is really important for applications).
%Look at TCP although other transport protocols certainly exist.


\section{Switch Architectures}
We consider two underlying switch hardware designs, a set of crossbar based switches with different scheduling policies, and a Benes network with a simple randomized routing scheme.% architectures, the traditional crossbar architecture and a set of modules connected in a benes network.   

For the crossbar switches the only differentiating component is the scheduler which determines which set of packets will be transmitted at each transmission opportunity.  A good scheduler can support an arbitrarily high load, and provides a low delay per packet.  In practice the scheduler will incur some overhead because it takes time to generate a feasible transmission schedule.  The schedulers we consider are allotted a small number of contention slots at the beginning of each packet transmission opportunity during which the inputs and the outputs can exchange information to determine the best schedule.  The communcation mechanism between the inputs and outputs can be quite powerful in practice.  We assume that each input is connected to each output by a bidirectional one bit line. This allows each output and input to exchange 1 bit in each contention slot.  After a schedule is generated the packets are transmitted and then the process is repeated.  %See figure \ref{con_slot_ill} for an illustration of crossbar switch operation.  

In contrast to the crossbar, the Benes network modules are non blocking and therefore can get away without needing an explicit scheduler.  Questions of how to route most effectively do raise themselves, however.  The reason that we chose the Benes network as the switch topology is because there is a very simple full throughput routing scheme, which takes advantage of the multiple paths offered to decrease congestion.


%Most of the ideas were inspired by Carrier Sensing Multiaccess Algorithms pioneered for ethernet applications.
\subsection{SLIP}

%Practical --> invented by Nick Mckeown --> Probably used in Real switches --> Difficult to prove that it is good, but really great simulation performance

SLIP is a very intuitive and practical scheme invented by Nick Mckeown \cite{McKeown}.  It has been implemented in real systems and yields extremely good performance.  It is difficult to prove anything about this switch architecture, but simulations show that it performs very well in many key loading regimes.\\

{\it How it works:}

The motivation behind SLIP was to develop a simple protocol that avoids the use of randomization in the scheduler.  SLIP achieves this by adding a very simple state to the switch.  Each input keeps track of the output to which it last transmitted a packet.  Similarly each output keeps track of the last input from which it received a packet.  Armed with this information,the switch follows these three phases in each contention slot:\\

{\bf SLIP Phases}
\begin{itemize}
\item 1) Every {\it unscheduled} input sends requests to the outputs for which it has at least one packet to deliver.
\item 2) Every {\it unscheduled} output admits the packet request from the input giving round robin priority from the last input it served ($s'$). i.e. first admitting requests from $s'+1$ then $s'+2$ etc.
\item 3) Every input schedules itself for the output that admitted it giving a round robin priority from the last output it transmitted through ($d'$).  i.e. first scheduling an admission from $d'+1$ then $d'+2$ etc.\\
\end{itemize}

After all the last contention slot the scheduled input and outputs transmit a single packet and update their state, after which the cycle continues.  Note that since the time it takes to transmit a packet depends on its length, there will be a lot of idle time in the system if packet length varies significantly.  In practice this is addressed by breaking packets up into small equal sized cells so scheduling can be done on a cell by cell basis as opposed to a packet by packet basis.  In our simulations we only consider packets of uniform size to simplify the comparison with other policies.  Note that both the Benes network based switches and the QCSMA based schedulers can support variable length packet sizes without a loss in performance.  The overhead necessary to split packets into smaller cells may be particularly detrimental in high speed implementations with jumbo frames.

\subsection{Ideal QCSMA}

%Throughput optimal  --> Invented/Explored by Libin and Jean? --> Difficult to Implement

QCSMA is a throughput optimal scheme inspired by Carrier Sense Multiple Access protocols developed for scheduling nodes in a wireless environment.  Because it was designed for wireless environments it assumes a much more restricted feedback model than that used by SLIP, which may be one of the reasons that the practical implementations perform worse than expected.  The only feedback traditional QCSMA assumes to be available takes the form of a broadcast to all inputs or outputs. This differs markedly from the crossbar where it is possible to provide individual feedback to each input and outputs.%, which we will revisit later.

Still ideal QCSMA, is guaranteed to be full throughput.  Unfortunately the delay performance of priority flows will depend on the backlog of the queues which they enter, which penalizes low rate flows.  The ideal QCSMA takes place in continuous time and so is not constrained by the number of contention slots allotted to the scheduler, and thus represents a performance bound for the other schedulers.\\

{\it How it works:}

At the beginning of each transmission opportunity, each input $s$ generates an exponential timer $X_{s,d}$ for each nonempty queue  destined for output $d$ (denoted as $Q_{s,d}$).  $X_{s,d}$ is distributed with rate $\lambda_{s,d} = (Q_{s,d})^{\alpha}$.  Where $\alpha > 0$ is a parameter to be chosen.  Input $s$ then requests access to output $d$ when timer $X_{s,d}$ expires. \\ %There will be no collisions  as this happens in continuous time.\\

{\bf Ideal QCSMA:}
\begin{itemize}
\item {\it unscheduled} input queue $Q_{s,d}$ generates timer value $X_{s,d}\sim \text{exp}(Q_{s,d}^\alpha)$
\item When $X_{s,d}$ expires schedule transmission from input $s$ to output $d$ unless $d$ or $s$ is already scheduled.
\item When all timers expire transmit packets.\\
\end{itemize}

Note the probability of a collision (two or more inputs requesting access to the same output simultaneously) is 0 when the timer values $X_{s,d}$ are continuous.  In simulations we assume that the timer resolution period takes no time, so this scheduling policy can act as an idealized performance benchmark.  This is impossible to implement, so it is necessary to develop a time slotted version that works in a finite number of discrete of contention slots.  Because we assume uniform packet sizes, and synchronized scheduling, all inputs compete for all outputs at the beginning of each transmission opportunity.  This may negatively impact observed performance.% although this is unimplementable.  Also note that QCSMA is inspired by Carrier Sense Multiple Access protocols which do not take advantage of the more fine grained feedback available in the switch context, and leveraged by SLIP.

\subsection{Slotted QCSMA} \label{naive_qcsma}

%Approximation of Ideal QCSMA  -->  Hope for similar performance -- Needed to be tweaked to 

In a real system it is not possible to implement continuous time counters, so it is necessary to approximate the continuous time scheduler with a discrete time system.  The scheduling will take place during a set of contention reserved for scheduling the transmission in each time slot.  Our first attempt to implement QCSMA was to approximate the exponential timers by geometric random variables determining in which contention slot the transmission requests should be made.

If each contention slot is assumed to have duration $\beta$, then the probability that an exponential timer of rate $Q_{s,d}^\alpha$ expires within a time slot is given by:
\begin{align}
p'_{s,d} =1-e^{\beta Q_{s,d}^\alpha} \approx \beta Q_{s,d}^\alpha
\end{align}
% in  geometric random variable used to approximate the exponential can approximated by:
If we take $p'_{s,d}$ to be the request probability of the input $s$ to output $d$, in each contention slot we see that, in the limit where the number of contention slots goes to infinity and $\beta$ goes to zero, this should yield the same results as the ideal continuous time algorithm.  This is obviously unrealistic in a real system, as the time devoted to scheduling will waste valuable transmission time.  In order to more accurately model a practical implementation we therefore limit ourselves to $m=6$ scheduling contention slots per transmission, and limit the aggressiveness of the request rate in order to limit the number of collisions by defining:
\begin{align}  \label{geo_p}
p_{s,d} =\min \left[ \beta Q_{s,d}^\alpha,p_{\text{cap}}\right]
\end{align}

Where we chose $p_{\text{cap}}=.8$.  This then yields the following practical scheme:\\

{\bf Slotted QCSMA:}
\begin{itemize}
\item In the first contention slot for the packet transmission opportunity each input queue calculates $p_{s,d}$
\item {\it unscheduled} input queue $Q_{s,d}$ requests output $d$ with probability $p_{s,d}$.
\item Output $d$ announces the outcome of requests to it.  If there is only one request, $d$ reports success.  Upon multiple requests, $d$ reports collision. For no requests, $d$ reports no transmission.
\item If the requests of input $s$ were successful to the outputs in the set $\mathcal{D}$, $s$ schedules itself to output $d$ with probability $\frac{Q_{s,d}}{\sum_{d\in \mathcal{D}}Q_{s,d}}$ (i.e. $s$ chooses among its transmission opportunities in proportion to the queue lengths)
\item Unsuccessful inputs update $p_{s,d}$ according to one of the schemes below, and re start requesting process\\
\end{itemize}

The limited number of contention slots coupled with the amount of competition due to uniform packet sizes makes it critical that collision resolutions are performed quickly and result in successful transmissions.  To facilitate this, we hybridize our algorithm by modifying the transmission probabilities according to an adaptive scheme first proposed by Hajek and van Loon \cite{Hajek_van_Loon}, which was proven to be optimal in wireless environments.  The base probabilities are multiplied by $(1.518,1.000,.559)$  in the event of (no transmission, success, collision).  While this does require a trinary broadcast at the end of each contention slot it is possible to implement the feedback with a binary broadcast as well.

Even with these tricks there is still a large gap between the ideal performance and that of time slotted version.  One solution is to leverage the more refined feedback of SLIP, which can resolve conflicts faster and yields a very effective alternative scheduler described in the following section. 


\subsection{Timer Simulating QCSMA}

The main hurdle in making QCSMA a reasonable scheduling algorithm is that we assume very few (e.g. $m=6$) contention slots dedicated to generating a schedule.  SLIP on the other hand performs very well with only a few time slots.  The main reason for this discrepancy is the different feedback structures used by the schedulers. Upon collisions among the input requests the outputs under the SLIP scheduler can differentiate which inputs collided and select a winner, while the QCSMA scheduler does not take advantage of this ability.  This guarantees at least one of the colliding flows to be scheduled when using SLIP.  In contrast, following a collision in the time slotted QCSMA outlined in the previous section, both inputs need to waste valuable contention slots re-requesting to resolve a winner.

	The ability of the output to differentiate which inputs collided and subsequently provide individual feedback provides the inspiration for the following QCSMA based scheduler.
	
	Using the individual feedback from the output it is possible implement an immediate resolution, in which the output randomly assigns a winner amongst the colliding input requests.  See figures \ref{orig_qcsma} and \ref{output_feedback}.  
	
\begin{figure}%%[h]
\includegraphics[width=40mm]{orig_qcsma.png}
\caption{Original (\S\ref{naive_qcsma}) contention resolution.  Upon collision inputs only learn a collision has occurred and need to retransmit in the next time slot.} 	\label{orig_qcsma}
\end{figure}
\begin{figure}%%[h]
	 \includegraphics[width=40mm]{output_feedback.png}
	\caption{With a more finegrained feedback, the output can randomly assign a winner.  This allows collisions to be resolved much faster.} 	\label{output_feedback}
\end{figure}
	
	Knowing exactly which inputs requested access during each contention slot allows the inputs to transmit a more sophisticated message to the outputs.  A natural use of the contention slots is to encode a binary number representing the time a geometric timer would have expired.  This idea is illustrated in figure \ref{timer_sim}.
	
\begin{figure}%%[h]
	 \includegraphics[width=80mm]{timer_sim.png}
	\caption{Because the outputs know the request history of the different inputs they can interpret the use of contention slots as representing timers expiring on a much larger set of virtual slots.} 	\label{timer_sim}
\end{figure}

	The major drawback of encoding a binary number in the contention slots is that when collisions do occur they cannot be resolved until the complete timer value has been transmitted.  While this reduces the number of collisions, it also limits the number of collisions that can be resolved using output feedback.
	
	The algorithm we finally settled on is a hybrid of the above ideas.  Instead of using all the contention slots to create an $m$ bit value for the timers, the scheme breaks the contention slots into $i$ virtual slots during which inputs transmit an $m/i$ bit number which the outputs can resolve by randomly assigning a winner, after which transmissions are scheduled.
	
	So in each virtual slot:\\
	 
{\bf Timer Simulating QCSMA:}
\begin{itemize}
\item Unscheduled input queue $Q_{s,d}$ generates timer value $X_{s,d}\in[0, 2^{m/i}-1]$ where  $X_{s,d}$ is distributed geometrically with parameter $p_{s,d}$ from equation (\ref{geo_p}).
\item Unscheduled output $d$ determines set $\mathcal{S}=\{s:X_{s,d}\leq{X_{j,d}}\forall j\}$ and uniformly picks a winner $s_d^*$
\item Inputs take the set $\mathcal{D}=\{d:s_d^*=s\}$ and schedule output $d$ with probability $\frac{Q_{s,d}}{\sum_{d\in \mathcal{D}}Q_{s,d}}$
\item Crossbar is scheduled for those inputs and outputs.\\
\end{itemize}

The process is repeated for the unscheduled inputs and outputs till the last of the $i$ virtual slots has passed.  Once this happens the scheduled packets are transmitted.  

This scheduling strategy significantly outperforms the simple time slotted QCSMA.  In the graphs to come we will be referring to this implementation when we discuss practical QCSMA.

\subsection{Benes}

%Different Architecture! $-->$ Easy to implement --> Modular so scales  --> Fat tree-esque --> $log(n)$ min hop count

The complexity of the `practical' QCSMA scheduler is admittedly quite high.  Not only is the hardware needed to support the scheduler's operation complicated, but another implementation hurdle is the need to find appropriate parameters.  Consequently, we also explore a very different type of switch which does not make use of a crossbar as the key component.  By using a Benes network (depicted in figure \ref{benes_top}) as the hardware at the heart of the switch we can still attain provably optimal throughput without the need for a cumbersome scheduler \cite{Walrand_Varaiya}.  By applying a very simple routing scheme it is easy to show that the switch can support any feasible load.  See figure \ref{benes_routing} for an illustration of the routing strategy.

%{\bf How to build it.  (Part of the Clos family of networks)  How to route it.}

%One of the main difference in the TCP implementation of the Benes module is that the acknowledgements for the benes network must pass through the entire network, which varies with the network congestion.  (In the QCSMA module the ack time is a constant 1, since it can make use of the crossbar immediately after finishing a transmission.)  The ack time is estimated using an exponential moving average.
\begin{figure}%%[h]
	 \includegraphics[width=80mm]{benes_top.png}
	\caption{An $8 \times 8$ Benes network, composed of $2\log(8)-1=5$ stages and $8/2=4$ $2\times 2$ full throughput modules per stage.} 	\label{benes_top}
\end{figure}


\begin{figure}%%[h]
	 \includegraphics[width=80mm]{benes_routing.png}
	\caption{Routing is randomized in the first half of the network (Before stage log(8) which is 3 in an $8\times8$ network).  Once a packet reaches the center stage of the network, there is exactly one path from every module to a given output.} 	\label{benes_routing}
\end{figure}

{\it How it works:}

The Benes module (a member of the family of Clos network) is constructed by recursively connecting Benes networks to each other in a repeating pattern.  Figure \ref{benes_top} depicts an $8\times8$ network, but this pattern can easily be extended to an $2^n \times 2^n$ module for any integer $n\geq2$.

The routing scheme is very similar to that used in Fat Trees using Valiant Load Balancing \cite{Fat_Trees} \cite{VLB} and is depicted in figure \ref{benes_routing}.  Routing is done on a per packet basis ({\it not per flow}).  In the first half of the network a module randomly picks an outgoing link for each packet and forwards it to the next module.  This continues till the packets reach the midpoint of the network (the $\log(n)$th stage).  From this stage there is only one path to reach any output and so the packet is forwarded along the appropriate path.  This will result in out of order packets at the destination, but we anticipate all paths to be similarly congested and so reordering should not be complex.   Because this scheme perfectly load balances across all available links it is believable that this might be throughput optimal.  In fact this can indeed be shown as in \cite{Walrand_Varaiya}.  \cite{Walrand_Varaiya} also contains more explicit details as to the construction and history of these networks.


%{\bf Stuff to mention:}
%\begin{itemize}
%\item 32 by 32
%\item time slotted
%\item  Typical datacenter flows (Inputs)
%\end{itemize}
\section{Simulations}
%modules subject to the same flow pattern.  This is intended to make the comparison between different scheduling policies and module types without worrying about inconsistencies in flow statistics.  The flows are meant to mimic traffic in real datacenters, so the total number of flows in the system is roughly 10\% of the switch capacity, consisting of 80\% low throughput, delay sensitive flows and 20\% high throughput, delay insensitive flows.  Because there are outliers in any configuration, only the best 95\% of flows are displayed in the graphs.
%
%There are two types of graphs displayed here: there are spread graphs such as figure \ref{ideal_spread}, which show the number of statistics per flow of the rates and delays in a given module, and distribution graphs such as figure \ref{ideal_dist}, which show the actual values of the flow and delay rates of the delay sensitive graphs. \\




To compare the delay performance of the different switch architectures we built a set of simulators in C++.  All the graphs were plotted for $32 \times 32$ switches (i.e. 32 inputs and 32 outputs).  All our simulations assume equal sized packets in order to facilitate a cleaner comparison.  Simulation time is discrete with the minimum time step being the duration of transmitting a packet across one hop. (i.e. across the crossbar or through one stage in the Benes module).  This means that the minimum time to traverse the crossbar based switches was 1 time step, whereas for the Benes switch it was $2\log(32)-1 = 9$ time steps.  In all the simulations we assumed there were $6$ contention slots used to generate the crossbar schedules.  We assumed that the schedule generation time was negligible compared to the packet transmission time.  The switch operation was simulated for $10^6$ time steps, in order to reach the steady state switch behavior.  Simulation inputs were flow patterns passing through the switch, which were identical for all switches. %during a given experiment in order to avoid  make the comparison between different scheduling policies and module types without worrying about inconsistencies in flow statistics.   

 The simulations fell into two categories: an open loop version meant to measure switch throughput, and a closed loop version meant to mimic TCP operation over the switch.  %We go into more depth in subsequent sections.
 
\subsection{Open Loop Simulations}

We explored the capacity of the different switch architectures by injecting a stream of packets at a given rate into the switch.  For the QCSMA algorithms we also had to do a search for good parameters, and settled on the values in table \ref{qcsma_parameters}.  Once the appropriate parameters had been chosen we generated figure \ref{delay_graphs} by performing multiple simulations by increasing the fraction $f$ of switch capacity from $.1$ to $.95$.  The load was generated by having flows from every input $s$ to every output $d$.  In each time step, each flow received a new packet with an i.i.d. probability $p=f/32$.

\begin{table}[ht] \caption{QCSMA Parameters} 
\centering 
\begin{tabular}{c c c}
 \hline\hline 
 Algorithm & $\beta$ & $\alpha$ \\
  [0.5ex] \hline 
   QCSMA&.2&.9  \\
   ideal  QCSMA&-&1  \\
   %DCSMA&?&?  \\
  [1ex] \hline 
  \end{tabular}
   \label{qcsma_parameters} 
\end{table}

%The delay performance of $32 \times 32$ switches were examined using parameters for the different scheduling algorithms as seen in table \ref{qcsma_param}, with $6$ contention slots being used to generate crossbar schedules.  The load was generated using i.i.d. arrivals to the different flows as in the previous section.  The relative performance can be seen in figure \ref{delay_graphs}.

Slip performs surprisingly well, on par with the ideal QCSMA.  The timer simulating QCSMA matches their performance up to a point.  The Benes network unfortunately needs to pay an overhead of $9$ time steps to traverse all the stages of the module, which corresponds to an i.i.d. load of $90\%$ for the other modules. 

\begin{figure}%[h]
	 \includegraphics[width=90mm]{delaythroughput.eps}
	\caption{Delay performance tradeoffs for iid packet inputs. The parameters for the QCSMA derived schedulers are taken from table \ref{qcsma_parameters}.} 	
	\label{delay_graphs}
\end{figure}


 Our expectation was that QCSMA would outperform SLIP when the queues had different statistics.  To test this the delay throughput curves where drawn under iid inputs (figure \ref{delay_graphs}) and under inputs in which $50\%$ of the flows following an iid process and $50\%$ following the bursty Markov process (figure \ref{mixed_loads}).   The bursty load was generated by using an ON-OFF Markov process which generates packets whenever it is in the 'ON' state.  The performance was found to be significantly affected for the all the switches.  As seen in figure \ref{mixed_loads} the bursty traffic significantly increased delays.  While the practical QCSMA finally differentiated itself from SLIP, it did no better than a $25\%$ reduction in delay.  In this very bursty regime the performance difference between SLIP and the Benes switch also significantly decreases.

\begin{figure}%[h]
	 \includegraphics[width=90mm]{mixed_loads.eps}
	\caption{Delay performance tradeoffs for mixed iid and bursty packet arrivals. Note the large difference in scale from figure \ref{delay_graphs} which uses the same choice of parameters.} 	
	\label{mixed_loads}
\end{figure}



%We also performed a set of experiments with different flow patterns with packets generated by a TCP protocol which received congestion feedback in the form of acknowledgment timings and ECN bits.



%In simulations used to measure the throughput delay characteristics of the different switches, the flows are open loop and inject packets independently of how many have been received by the desired output.   Type 1 flows are meant to model bursty flows, which are modeled as on-off markov chains which injects a packet every timeslot in which the chain is in the on state.  Type 2 flows generate packets with the same iid probability independently of whether a packet was sent in the previous time slot.

%{\bf Throughput VS Delay Input Flows:}
%\begin{itemize}
%\item iid
%\item Markov Chains
%\item iid and Markov
%\end{itemize}



\subsection{Closed Loop (TCP) Simulations}
\begin{figure}%[h]
	 \includegraphics[width=90mm]{typical_flows.png}
	\caption{This graph represents the flow matrix of a `typical' datacenter load.  Where $80\%$ of the flows (type 2) generate a low volume of traffic and are sensitive to delays.  The other $20\%$ of flows (type 1) generate a large volume of traffic and are require a high throughput but not necessarily low delays.  The color in the $s,d$th position signifies the type of flow going from input $s$ to output $d$.  Flows of type $0$ do not generate any packets.} 	
	\label{typical_flows}
\end{figure}
%
%\begin{figure}%[h]
%	 \includegraphics[width=90mm]{multicast3.png}
%	\caption{This graph represents the flows generated by a three different spread and aggregate applications living on servers 1, 5, and 11.}
%	\label{multicast3}
%\end{figure}



As the throughput supported by the different switches seems comparable, we focus on the delay performance of the switches under TCP connections.  TCP Inputs to the simulator can be represented as a flow type matrix such as the one visualized in figure \ref{typical_flows}.  The entry $s,d$ represents the type of flow from input $s$ to output $d$.  We consider two main types of flows, high throughput flows that are relatively insensitive to delay (type 1 flows) and low throughput flows that are sensitive to delays (type 2 flows).  The packets generated for TCP transmission are generated according to Markov ON OFF processes much as in the open loop simulations with one producing a very low throughput set of packets and the other being generating a much higher throughput, in order to simulate the different types of datacenter flows.  The type 1 process has transitions from ON to OFF transition with probability $.3$ and from OFF to ON with probability $.7$.  The type 2 process transitions from ON to OFF with probability $0.0781$ and from OFF to ON with probability $0.9219$.  When the process is ON it generates packets, and it does not when it is OFF.   State transitions occur at each time step.

The TCP window size is updated by additive increase multiplicative decrease, with feedback coming in the form of acks that are generated whenever a packet successfully traverses the switch.  The acks in our simulation are not actually packets sent through our network, but rather simply notify our TCP simulator after an artificially determined delay.  After an ack notifies the TCP flow it adjusts its send window statistics accordingly.  The ack is delayed by a factor of 10 times the delay it took the packet to traverse the switch in order to simulate the round trip time in a datacenter that has five switches between the input and its intended output.\\


\subsubsection{Parameter Search}

In our model there is no maximum buffer size, and consequently no dropped packets.  Consequently, we need to give TCP feedback on when to halve its window size by another means.  We do this by using the explicit congestion control bit in the tcp header.  Our simulations mark packets as facing congestion with a probability $p_{\text{ECN}}$ if the packets arrive at a queue at or exceeding a threshold $T_{\text{ECN}}$ packets.  After our TCP receives an ack which announces that it faced congestion, it will halve it's window, and wait one round trip time before rehalving it's window, in order to flush out all the duplicate congestion announcements due to packets sent before the congestion announcement was received.

  TCP should be optimized for its environment so we conducted a parameter search to determine good values for each of the switches.  While the QCSMA was tuned to perform well for a worst case scenario, for TCP we are interested in improving average performance.  The search for the two parameters $p_{\text{ECN}}$ and $T_{\text{ECN}}$ is conducted over what we consider an average set of average flow statistics for a datacenter, as suggested in \cite{Benson} and \cite{Kandula} and shown in figure \ref{typical_flows}.

%As we assume the new datacenters will be built with a single switch architecture in mind, so we search for good TCP parameters for each switch alternative.  
Please see table \ref{ecn_table} for the choices we settled on.  Once we settled on the parameters for TCP we can compare how the different architectures compare with each other for a variety of trial flows.  
%See the figures \ref{tcp_perf}.


We present 4 simulations in which we simulated different flow patterns to our switch architectures and observed the average delay per flow.  The flow patterns we investigated were:

\begin{itemize}
\item all to all 
\item spread and aggregate 
%\item three simultaneous spread and aggregate
\item typical 
\item typical and spread and aggregate \\
\end{itemize}

\subsubsection{All to all (core switch)}
An all to all pattern can be seen in figure \ref{all_to_all}.  The type of each flow (from $s$ to $d$) was drawn from an iid distribution that yielded type 1 with probability $1/2$ and type 2 with probability $1/2$.  This flow pattern was meant to mimic a possible load on a core switch, which has to aggregate the traffic from across the network.  Results suggest that the performance of the switches was very similar.  The throughput for all the switches is about the same, and in terms of delay the difference is not significant. Ideal QCSMA performs mildly better than the other switches and Benes performs the least consistently across different flows.  See figure \ref{all_to_all_delay} for a comparison.

Notably the gain of the unimplementable ideal QCSMA is negligible compared to that of SLIP, which does not have the same strong performance guarantees on throughput.  This will be a recurring theme in our findings.  Our implementation of a practical QCSMA, is comparable to SLIP but does perform slightly worse than SLIP.

\begin{figure}%[h]
	 \includegraphics[width=90mm]{all_to_all.png}
	\caption{This flow matrix is meant to model the traffic in a core switch.  Every input is connected to every output and the traffic carried is mixed between type 1 and type 2 flows.}
	\label{all_to_all}
\end{figure}

\begin{figure}%[h]
	 \includegraphics[width=90mm]{all_to_all_delay.eps}
	\caption{The per flow delay spread for the different switch architectures under an all to all flow pattern.  Only the best 95\% of flows are shown to remove outliers.  The blue bar represents the spread between the best and worst flows and the red line represents the average value.  Clockwise from the top left we have timer simulating QCSMA, SLIP, Benes, and Ideal QCSMA.}
	\label{all_to_all_delay}
\end{figure}

\begin{table}[ht] \caption{TCP Parameters} 
\centering 
\begin{tabular}{c c c}
 \hline\hline 
 Switch & $T_{\text{ECN}}$ & $p_{\text{ECN}}$ \\
  [0.5ex] \hline 
  Benes&23&.5 \\
 SLIP&10&.5 \\
  QCSMA&12&.5  \\
   ideal  QCSMA&10&.9  \\
   %DCSMA&31&25  \\
  [1ex] \hline 
  \end{tabular}
   \label{ecn_table} 
\end{table}


\subsubsection{Spread and Aggregate (MapReduce)}
One of the most famous (if not most popular) applications in modern datacenters is certainly MapReduce.  The communication pattern of MapReduce often involves a large transfer of data to and from a single node between the different phases of computation.  In order to mimic this type of operation we developed a spread and aggregate flow pattern in which a given server sends data to all other servers using low throughput highly delay sensitive flows (type 2), which in turn are returned to the server.   An example in which the source of the pattern is on input 1 is shown in figure \ref{spreadagg}

This does not accurately model a MapReduce job for several reasons.  The biggest departure from a typical MapReduce job is that our type 2 flows do not have similar synchronization or burstiness properties due to the similar completion times of computations in a real MapReduce job.  Furthermore the large delays imposed by the artificial 10 hop distance for the acks is responsible for another difference in our model.  Still we hope that it suffices to give an impression of how a spread and aggregate application might behave.

The performance for the different crossbar applications is comparable as can be seen in figure \ref{spreadagg_delay_spread}, but the Benes network yields large delays as well as a larger variation in performance of different flows when supporting this type of workload.
%\begin{figure}%[h]
%	 \includegraphics[width=90mm]{multicast.png}
%	\caption{This graph represents flows generated by having typical datacenter flow and two multicast applications living on servers 1 and 2.}
%	\label{multicast}
%\end{figure}

%\begin{figure}%[h]
%	 \includegraphics[width=90mm]{typ_and_mult2.png}
%	\caption{This graph represents the flows generated by having typical datacenter flow and two multicast applications living on servers 1 and 2.}
%	\label{typ_and_mult2}
%\end{figure}
\begin{figure}%[h]
	 \includegraphics[width=90mm]{spreadagg.png}
	\caption{This graph represents the flow matrix of a spread and aggregate pattern originating from input 1.  All flows are of type 2.}
	\label{spreadagg}
\end{figure}

\begin{figure}%[h]
	 \includegraphics[width=90mm]{spreadagg_delay_spread.eps}
	\caption{This graph shows the delay spread of the flows under a simulated spread and aggregate pattern.}
	\label{spreadagg_delay_spread}
\end{figure}

%\begin{figure}%[h]
%	 \includegraphics[width=90mm]{spreadagg_flow_delays.png}
%	\caption{This graph represents the performance generated by a `typical' datacenter load with $80\%$ low throughput delay sensitive flows and $20\%$ high throughput delay insensitive flows.}
%	\label{spreadagg_flow_delays}
%\end{figure}

\subsubsection{`Typical' datacenter flows}
The statistics in \cite{Benson} and \cite{Kandula} suggest that it is normal to have a relatively low utilization of the networks in modern datacenters.  In their findings they observed that only $10\%$ of the servers communicate.  They observed that $80\%$ of the flows had very short durations, while $20\%$ flows were really long lived and responsible for the majority of transmitted bits.  To model the bimodal behavior they observed we choose to use the two types of flows mentioned above.  A sample corresponding flow pattern for our simulations can be seen in figure \ref{typical_flows}.


In our simulations in this scenario all the crossbar based switches once again exhibited comparable performance (see figure \ref{typ_delays}).  The Benes network is disadvantaged by the minimum delay imposed by the need of each packet to traverse all the stages of the switch, but seems otherwise to have similar delay characteristics except for the 9 time step shift.

Another notable difference between the crossbar and Benes based switches is that the variation in the delay of the low throughput flows is large in the crossbar based switches, while it is the same as the high throughput traffic in the Benes network.  For the QCSMA based schemes this may be because the time it takes an input access an output link is proportional to the backlog of the queues waiting to use it.  This means low throughput flows will face a delay penalty when they contend with high throughput flows.  In the Benes network there is no blocking for the transmissions, so packets are only impeded by the backlogs they face in the switches, and the randomized routing means that this will be approximately equal for all inputs.

%\begin{figure}%[h]
%	 \includegraphics[width=90mm]{typmult2flow.png}
%	\caption{This graph represents the flows generated by a three different multicast applications living on servers 1, 5, and 11.}
%	\label{typmult2flow}
%\end{figure}

\begin{figure}%[h]
	 \includegraphics[width=90mm]{typ_delays.eps}
	\caption{This graph represents the performance generated by a `typical' datacenter load with $80\%$ low throughput delay sensitive flows (type 2) and $20\%$ high throughput delay insensitive flows (type 1).}
	\label{typ_delays}
\end{figure}

\subsubsection{`Typical' Flows and spread and aggregate}
Finally to explore switch performance in a datacenter setting that supports both  `typical' datacenter loads and some MapReduce like jobs, we subjected the switch to a superposition of the datacenter load and two spread and aggregate loads on servers 1 and 11 as seen in figure \ref{typ_mult2}.  The performance of the crossbar based switches is again comparable (see figure \ref{typ_mult2_delays}), while the Benes switch exhibits significantly higher delays. The largest delays in the Benes switch taking more than three times as long as the worst crossbar design.

As can be seen in figure \ref{mixed_delay_matrix} the long delays are due to the spread and aggregate flows for all the switches, but for the Benes network they do especially badly.  Note that since it is the spread and aggregate flows (all type 2 flows) that suffer, figure \ref{typ_mult2_delays} shows type 2 as being particularly slow.  This is only true because the spread and aggregate flows are heavily delayed as opposed to the spread and aggregate flows slowing down other type 2 flows.  The other very interesting observation to make is that the incast flows in the Benes switch actually fair pretty well.  It is the many to one transmission that cannot take full advantage of the multipath and therefore incurs the large delays.
\begin{figure}%[h]
	 \includegraphics[width=90mm]{mult2.png}
	\caption{This graph represents the mix of a `typical' load and two spread and aggregate jobs.}
	\label{typ_mult2}
\end{figure}

\begin{figure}%[h]
	 \includegraphics[width=90mm]{mult2_flow_delays.eps}
	\caption{This graph represents the performance under a mix of spread and aggregate (on servers 1 and 11) and typical flows.}
	\label{typ_mult2_delays}
\end{figure}

\begin{figure}%[h]
	 \includegraphics[width=90mm]{mult2_delay_matrix.png}
	\caption{This graph shows the average delay per flow for the mix of `typical' and spread and aggregate inputs.  The inputs originating the spread and aggregate flows (1 and 11) face the largest delays in the crossbar graphs, and here finally the ideal QCSMA differentiates itself from SLIP, but the practical QCSMA performance still remains comparable to SLIP.}
	\label{mixed_delay_matrix}
\end{figure}
%\begin{figure}%[h]
%	 \includegraphics[width=90mm]{typ.png}
%	\caption{This graph represents the flows generated by a `typical' data center load with $80\%$ low throughput delay sensitive flows and $20\%$ high throughput delay insensitive flows.}
%	\label{typ}
%\end{figure}
%
%\begin{figure}%[h]
%	 \includegraphics[width=90mm]{typflow.png}
%	\caption{This graph represents the flows generated by a `typical' data center load with $80\%$ low throughput delay sensitive flows and $20\%$ high throughput delay insensitive flows.}
%	\label{typflow}
%\end{figure}
%
%\begin{figure}%[h]
%	 \includegraphics[width=90mm]{mult1flow.png}
%	\caption{This graph represents the flows generated by a one spread and aggregate application living on servers 1.}
%	\label{mult1flow}
%\end{figure}
%

\section{Conclusion}
The goal of this paper was to explore alternatives to the state of the art switch technologies (SLIP) and determine whether a significant performance gain could be made using some of the new throughput optimal switch architectures.  Our simulations suggest that SLIP performs extremely well in all the scenarios we considered, outperforming our practical implementations of both QCSMA and Benes network inspired switches in almost all cases.    The best performance gain of the implementable QCSMA that we observed was a $25\%$ improvement in delay over SLIP.   The performance gain of the provably optimal ideal QCSMA only becomes noticeable in a single type of load.  While the guarantees on SLIP's supportable throughput are much weaker than that of ideal QCSMA and the Benes switch, it seems to have a better real throughput than our implementable QCSMA algorithm under symmetric conditions.  Under assymmetric loads the performance improvement does not seem to be significant.  Given the loading profiles that we see in datacenters, it seems unlikely that this will become a problem in practice.

Benes networks as switches perform well; with no complex scheduling machinery necessary and very strong guarantees on throughput performance.  There is a problem with the minimum packet delays due to the minimum number of hops to traverse the switch, but perhaps with a cut through architecture this could be significantly reduced.  As switches continue to get larger, the overhead becomes a smaller and smaller ($2\log(n)-1$ for an $n\times n$ switch), which may make this a practical implementation in the future.  Especially as it's modular nature makes it very scalable to the needs of the network.

Our simplifying assumptions on the scheduling overheads are too strong to draw decisive conclusion, yet.  The assumption that all the packets are of equal size and are scheduled synchronously is the best case for SLIP and the worst for QCSMA.  The limitation on contention slots, also breaks down the performance guarantees of QCSMA.  In order to better evaluate this we will need to modify our simulations to take this into account. Our future work will focus on a more precise modeling of the effects introduced by asynchronous scheduling available to QCSMA and the effect of variable packet sizes on SLIP performance.

%One of the nice aspects of the Benes network as a network building block is its modular nature which makes it easy to create bigger switches.

% Good performance and no scheduling machinery but, needs to pay hop overhead.  QCSMA seems to perform well, but the implementations considered haven't managed to outperform SLIP in very natural scenarios. In fact ideal QCSMA doesn't outperform SLIP by much and so the added scheduling complexity seems unwarranted.  The need for optimizing parameters for slotted QCSMA is disturbing as it makes it difficult to build one switch well suited for all scenarios.


%\section{To do}
%\begin{itemize}
%\item spread and aggregate
%\item Crossbar Capabilities
%\item standardize $m$s and $n$s
%\end{itemize}


%\appendix{}
%\subsection{Scheduler Parameter Search}
%\subsubsection{Ideal QCSMA}
%The search for parameters of the ideal QCSMA algorithm was limited to finding the value of $\alpha$ since the value of $\beta$ does not have any effect in continuous time.  As it turned out, $\alpha =1$ worked best.
%
%
%
%
%\subsubsection{Practical QCSMA}
%Unfortunately there are a lot of parameters to be tuned in the search for a good implementation of the non-ideal QCSMA algorithm.  To find good choices of the parameters $\beta$ and $\alpha$ for a $32\times 32$ module we compared the delay performance of the switch loaded at $80\%$ capacity.  The reason for choosing this capacity was as an upper bound of a likely operating regime.  The $80\%$ load was generated by having flows from every input $s$ to every output $d$.  In each time slot, each flow received a new packet with an i.i.d. probability $p=.8/32$.  A sample curve from the parameters sampling can be seen in figure \ref{qcsma_param}.
%
%
%\begin{figure}%[h]
%	 \includegraphics[width=90mm]{qcsma_param.eps}
%	\caption{The delay (measured in packet transmission time slots) performance improves drastically with increasingly large choices of $\beta$, but is much less sensitive to the choice of $\alpha$} 	
%	\label{qcsma_param}
%\end{figure}
%
%
%In order to preserve the queue awareness we chose the value $\alpha =.9$.  The value of $\beta=.2$ was chosen to be large but to still have some resolution in terms of queue lengths.



\label{bib}
\bibliography{ClassProject}{}
\bibliographystyle{plain} 
 
\end{document}